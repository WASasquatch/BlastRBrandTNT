{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WASasquatch/BlastRBrandTNT/blob/master/Stability_AI_Easy_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6RXjS1tTji"
      },
      "source": [
        "# Stability.AI Easy Diffusion v0.1 ![visitors](https://visitor-badge.glitch.me/badge?page_id=EasyDiffusion&left_color=blue&right_color=orange) [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://raw.githubusercontent.com/WASasquatch/easydiffusion)\n",
        "\n",
        "A fork of NOP's Stable Diffusion Colab \n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G76cGaiuGdjJ"
      },
      "source": [
        "## Information\n",
        "\n",
        "Changelog:\n",
        "- v0.1: Forked [NOP's Stable Diffusion Colab v0.23](https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17?usp=sharing)\n",
        "  - Added File Prompts\n",
        "  - Added Noodle Soup Prompts\n",
        "\n",
        "<br>\n",
        "\n",
        "## Stablity.AI Model Terms of Use\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "\n",
        "1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "\n",
        "2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "\n",
        "3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOEF-K5F5db"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ekR-LW6trWG"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU Status\n",
        "#@markdown Check the status of the allocated GPU\n",
        "import subprocess\n",
        "!nvidia-smi\n",
        "nvidiasmi_simple = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "gpu_name = nvidiasmi_simple.split(':')[1].split('(')[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "P8gV4-qRDn1b"
      },
      "outputs": [],
      "source": [
        "#@title Setup Environment\n",
        "\n",
        "# Import future print\n",
        "from __future__ import print_function\n",
        "try:\n",
        "    import __builtin__\n",
        "except ImportError:\n",
        "    import builtins as __builtin__\n",
        "\n",
        "# Emoticon fun!\n",
        "import subprocess\n",
        "try:\n",
        "    import emoji\n",
        "except ImportError:\n",
        "     multipip_res = subprocess.run(['pip', '-q', 'install', 'emoji'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "finally:\n",
        "    import emoji\n",
        "\n",
        "# Override Print Function\n",
        "def print(message, *args, **kwargs):\n",
        "    if 'defaultprint' in kwargs:\n",
        "        kwargs.pop('defaultprint')\n",
        "        return __builtin__.print(message, *args, **kwargs)\n",
        "    else:\n",
        "        return __builtin__.print(emoji.emojize(message), *args, **kwargs)\n",
        "\n",
        "\n",
        "import os, torch, gc, requests, io\n",
        "\n",
        "def gitclone(url, targetdir=None):\n",
        "    if targetdir:\n",
        "        res = subprocess.run(['git', 'clone', '--quiet', url, targetdir], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    else:\n",
        "        res = subprocess.run(['git', 'clone', '--quiet', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipi(modulestr):\n",
        "    res = subprocess.run(['pip', 'install', modulestr + ' --quiet'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipie(modulestr):\n",
        "    res = subprocess.run(['git', 'install', '-e', modulestr + ' --quiet'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', '-q', '--show-progress', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def clean_env():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "def clear():\n",
        "    from IPython.display import clear_output; return clear_output()\n",
        "\n",
        "# Basic image display\n",
        "def displayJsImage(name, img):\n",
        "    from IPython.display import display, Javascript, clear_output\n",
        "    from google.colab.output import eval_js\n",
        "    from base64 import b64encode\n",
        "    from google.colab import files\n",
        "    img = np.asarray(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    js = Javascript('''\n",
        "        async function showImage(name, image, width, height) {\n",
        "        img = document.getElementById(name);\n",
        "        cont = document.getElementById(name+'_container');\n",
        "        if(img == null && cont == null) {\n",
        "            cont = document.createElement('div');\n",
        "            cont.id = name+'_container';\n",
        "            link = document.createElement('a');\n",
        "            link.href = image;\n",
        "            link.target = '_blank';\n",
        "            img = document.createElement('img');\n",
        "            img.id = name;\n",
        "            img.class = \"resultImage\"\n",
        "            cont.style = 'display:inline-block;width:auto;text-align:center;font-size:14px;font-weight:bold;background-color:rgba(0,0,0,0.1);border-radius:5px;padding:2px;margin:2px;'\n",
        "            document.body.appendChild(cont)\n",
        "            cont.innerHTML = '<p style=\"margin:3px auto;width:180px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;\">'+name+'</p>';\n",
        "            cont.appendChild(link);\n",
        "            link.appendChild(img);\n",
        "        }\n",
        "        img.src = image;\n",
        "        //img.width = (width / 2);\n",
        "        //img.height = (height / 2);\n",
        "        img.style = \"margin: 5px; vertical-align: text-top; max-width: 256px; max-height: 512px;\";\n",
        "        }\n",
        "    ''')\n",
        "    height, width = img.shape[:2]\n",
        "    ret, data = cv2.imencode('.png', img)\n",
        "    data = b64encode(data)\n",
        "    data = data.decode()\n",
        "    data = 'data:image/png;base64,' + data\n",
        "    display(js)\n",
        "    eval_js(f'showImage(\"{name}\", \"{data}\", {width}, {height})')\n",
        "\n",
        "try:\n",
        "  with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "    key = f.read().decode('utf-8').split(':')\n",
        "except OSError as e:\n",
        "  print(e)\n",
        "  \n",
        "huggingface_username = key[0].strip()\n",
        "huggingface_token = key[1].strip()\n",
        "\n",
        "clean_env()\n",
        "\n",
        "LOW_VRAM_PATCH = True #@param {type:\"boolean\"}\n",
        "#@markdown `LOW_VRAM_PATCH`: Use 16bit float instead of 32bit float. This saves VRAM, at the potential cost of model fidelity.<br>**Note:** You may need this if you're using a GPU with ~16GB VRAM.<br>\n",
        "\n",
        "#@markdown ---\n",
        "ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "#@markdown `ENABLE_NSFW_FILTER`: Enable NSFW censoring\n",
        "\n",
        "#@markdown ---\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "#@markdown Use Google Drive to store images and prompt information\n",
        "\n",
        "#@markdown ---\n",
        "CLEAR_SETUP_LOG = True #@param{type: 'boolean'}\n",
        "#@markdown Clear the setup log after installation compeltes.\n",
        "\n",
        "# Enable third-party widgets\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "OUTDIR = '/content/diffusers_output'\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "  !pip install shutup\n",
        "  import shutup; shutup.please()\n",
        "\n",
        "  !git lfs install\n",
        "  !GIT_LFS_SKIP_SMUDGE=0\n",
        "  # This will take a while\n",
        "  !git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "  !pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "  # Disable NSFW check\n",
        "  if not ENABLE_NSFW_FILTER:\n",
        "    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:\n",
        "      file.write('''\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\n",
        "\n",
        "from ...utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "def cosine_distance(image_embeds, text_embeds):\n",
        "    normalized_image_embeds = nn.functional.normalize(image_embeds)\n",
        "    normalized_text_embeds = nn.functional.normalize(text_embeds)\n",
        "    return torch.mm(normalized_image_embeds, normalized_text_embeds.T)\n",
        "\n",
        "\n",
        "class StableDiffusionSafetyChecker(PreTrainedModel):\n",
        "    config_class = CLIPConfig\n",
        "\n",
        "    def __init__(self, config: CLIPConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.vision_model = CLIPVisionModel(config.vision_config)\n",
        "        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n",
        "\n",
        "        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n",
        "        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n",
        "\n",
        "        self.register_buffer(\"concept_embeds_weights\", torch.ones(17))\n",
        "        self.register_buffer(\"special_care_embeds_weights\", torch.ones(3))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, clip_input, images):\n",
        "        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n",
        "        image_embeds = self.visual_projection(pooled_output)\n",
        "\n",
        "        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().numpy()\n",
        "        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().numpy()\n",
        "\n",
        "        result = []\n",
        "        batch_size = image_embeds.shape[0]\n",
        "        for i in range(batch_size):\n",
        "            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n",
        "\n",
        "            # increase this value to create a stronger `nfsw` filter\n",
        "            # at the cost of increasing the possibility of filtering benign images\n",
        "            adjustment = 0.0\n",
        "\n",
        "            for concet_idx in range(len(special_cos_dist[0])):\n",
        "                concept_cos = special_cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.special_care_embeds_weights[concet_idx].item()\n",
        "                result_img[\"special_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"special_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"special_care\"].append({concet_idx, result_img[\"special_scores\"][concet_idx]})\n",
        "                    adjustment = 0.01\n",
        "\n",
        "            for concet_idx in range(len(cos_dist[0])):\n",
        "                concept_cos = cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.concept_embeds_weights[concet_idx].item()\n",
        "                result_img[\"concept_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"concept_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"bad_concepts\"].append(concet_idx)\n",
        "\n",
        "            result.append(result_img)\n",
        "\n",
        "        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n",
        "\n",
        "        #for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n",
        "        #    if has_nsfw_concept:\n",
        "        #        images[idx] = np.zeros(images[idx].shape)  # black image\n",
        "\n",
        "        if any(has_nsfw_concepts):\n",
        "            logger.warning(\n",
        "                \"Potential NSFW content was detected in one or more images, but the NSFW filter is off.\"\n",
        "            )\n",
        "\n",
        "        return images, has_nsfw_concepts''')\n",
        "  !pip install transformers\n",
        "  # make sure you're logged in with `huggingface-cli login`\n",
        "  from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "\n",
        "  # lms = LMSDiscreteScheduler(\n",
        "  #     beta_start=0.00085, \n",
        "  #     beta_end=0.012, \n",
        "  #     beta_schedule=\"scaled_linear\"\n",
        "  # )\n",
        "\n",
        "  model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "  !mkdir diffusers_output\n",
        "  !pip install pytorch-pretrained-bert\n",
        "  !pip install spacy ftfy\n",
        "  !python -m spacy download en\n",
        "  !pip install scipy\n",
        "  !git clone --recursive https://github.com/crowsonkb/k-diffusion.git\n",
        "  !echo $huggingface_token | huggingface-cli login\n",
        "  if LOW_VRAM_PATCH:\n",
        "    patched_file = open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py').read().replace('input, self.num_groups, self.weight, self.bias, self.eps)','input, self.num_groups, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)')\n",
        "    with open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py','w') as file:\n",
        "      file.write(patched_file)\n",
        "    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py','w') as file:\n",
        "      file.write(\n",
        "  '''\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from ...models import AutoencoderKL, UNet2DConditionModel\n",
        "from ...pipeline_utils import DiffusionPipeline\n",
        "from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from .safety_checker import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vae: AutoencoderKL,\n",
        "      text_encoder: CLIPTextModel,\n",
        "      tokenizer: CLIPTokenizer,\n",
        "      unet: UNet2DConditionModel,\n",
        "      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "      safety_checker: StableDiffusionSafetyChecker,\n",
        "      feature_extractor: CLIPFeatureExtractor,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      scheduler = scheduler.set_format(\"pt\")\n",
        "      self.register_modules(\n",
        "          vae=vae,\n",
        "          text_encoder=text_encoder,\n",
        "          tokenizer=tokenizer,\n",
        "          unet=unet,\n",
        "          scheduler=scheduler,\n",
        "          safety_checker=safety_checker,\n",
        "          feature_extractor=feature_extractor,\n",
        "      )\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(\n",
        "      self,\n",
        "      prompt: Union[str, List[str]],\n",
        "      height: Optional[int] = 512,\n",
        "      width: Optional[int] = 512,\n",
        "      num_inference_steps: Optional[int] = 50,\n",
        "      guidance_scale: Optional[float] = 7.5,\n",
        "      eta: Optional[float] = 0.0,\n",
        "      generator: Optional[torch.Generator] = None,\n",
        "      output_type: Optional[str] = \"pil\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "      if \"torch_device\" in kwargs:\n",
        "          device = kwargs.pop(\"torch_device\")\n",
        "          warnings.warn(\n",
        "              \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "              \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "          )\n",
        "\n",
        "          # Set device as before (to be removed in 0.3.0)\n",
        "          if device is None:\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "          self.to(device)\n",
        "\n",
        "      if isinstance(prompt, str):\n",
        "          batch_size = 1\n",
        "      elif isinstance(prompt, list):\n",
        "          batch_size = len(prompt)\n",
        "      else:\n",
        "          raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "      if height % 8 != 0 or width % 8 != 0:\n",
        "          raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "      # get prompt text embeddings\n",
        "      text_input = self.tokenizer(\n",
        "          prompt,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "      # corresponds to doing no classifier free guidance.\n",
        "      do_classifier_free_guidance = guidance_scale > 1.0\n",
        "      # get unconditional embeddings for classifier free guidance\n",
        "      if do_classifier_free_guidance:\n",
        "          max_length = text_input.input_ids.shape[-1]\n",
        "          uncond_input = self.tokenizer(\n",
        "              [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "          )\n",
        "          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "          # For classifier free guidance, we need to do two forward passes.\n",
        "          # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "          # to avoid doing two forward passes\n",
        "          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      # get the intial random noise\n",
        "      latents = torch.randn(\n",
        "          (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "          generator=generator,\n",
        "          device=self.device,\n",
        "      )\n",
        "      latents = latents.half()\n",
        "\n",
        "      # set timesteps\n",
        "      accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "      extra_set_kwargs = {}\n",
        "      if accepts_offset:\n",
        "          extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "      if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "          latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "      # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "      # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "      # and should be between [0, 1]\n",
        "      accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "      extra_step_kwargs = {}\n",
        "      if accepts_eta:\n",
        "          extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "      for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "          # expand the latents if we are doing classifier free guidance\n",
        "          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              sigma = self.scheduler.sigmas[i]\n",
        "              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "          # predict the noise residual\n",
        "          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "          # perform guidance\n",
        "          if do_classifier_free_guidance:\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "          # compute the previous noisy sample x_t -> x_t-1\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "          else:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "      # scale and decode the image latents with vae\n",
        "      latents = 1 / 0.18215 * latents\n",
        "      image = self.vae.decode(latents)\n",
        "\n",
        "      image = (image / 2 + 0.5).clamp(0, 1)\n",
        "      image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "      # run safety checker\n",
        "      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "      if output_type == \"pil\":\n",
        "          image = self.numpy_to_pil(image)\n",
        "\n",
        "      return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}\n",
        "  ''')\n",
        "\n",
        "    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py', 'w') as file:\n",
        "      file.write(\n",
        "  '''\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from ...models import AutoencoderKL, UNet2DConditionModel\n",
        "from ...pipeline_utils import DiffusionPipeline\n",
        "from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from .safety_checker import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vae: AutoencoderKL,\n",
        "      text_encoder: CLIPTextModel,\n",
        "      tokenizer: CLIPTokenizer,\n",
        "      unet: UNet2DConditionModel,\n",
        "      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "      safety_checker: StableDiffusionSafetyChecker,\n",
        "      feature_extractor: CLIPFeatureExtractor,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      scheduler = scheduler.set_format(\"pt\")\n",
        "      self.register_modules(\n",
        "          vae=vae,\n",
        "          text_encoder=text_encoder,\n",
        "          tokenizer=tokenizer,\n",
        "          unet=unet,\n",
        "          scheduler=scheduler,\n",
        "          safety_checker=safety_checker,\n",
        "          feature_extractor=feature_extractor,\n",
        "      )\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(\n",
        "      self,\n",
        "      prompt: Union[str, List[str]],\n",
        "      height: Optional[int] = 512,\n",
        "      width: Optional[int] = 512,\n",
        "      num_inference_steps: Optional[int] = 50,\n",
        "      guidance_scale: Optional[float] = 7.5,\n",
        "      eta: Optional[float] = 0.0,\n",
        "      generator: Optional[torch.Generator] = None,\n",
        "      output_type: Optional[str] = \"pil\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "      if \"torch_device\" in kwargs:\n",
        "          device = kwargs.pop(\"torch_device\")\n",
        "          warnings.warn(\n",
        "              \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "              \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "          )\n",
        "\n",
        "          # Set device as before (to be removed in 0.3.0)\n",
        "          if device is None:\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "          self.to(device)\n",
        "\n",
        "      if isinstance(prompt, str):\n",
        "          batch_size = 1\n",
        "      elif isinstance(prompt, list):\n",
        "          batch_size = len(prompt)\n",
        "      else:\n",
        "          raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "      if height % 8 != 0 or width % 8 != 0:\n",
        "          raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "      # get prompt text embeddings\n",
        "      text_input = self.tokenizer(\n",
        "          prompt,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "      # corresponds to doing no classifier free guidance.\n",
        "      do_classifier_free_guidance = guidance_scale > 1.0\n",
        "      # get unconditional embeddings for classifier free guidance\n",
        "      if do_classifier_free_guidance:\n",
        "          max_length = text_input.input_ids.shape[-1]\n",
        "          uncond_input = self.tokenizer(\n",
        "              [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "          )\n",
        "          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "          # For classifier free guidance, we need to do two forward passes.\n",
        "          # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "          # to avoid doing two forward passes\n",
        "          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      # get the intial random noise\n",
        "      latents = torch.randn(\n",
        "          (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "          generator=generator,\n",
        "          device=self.device,\n",
        "      )\n",
        "      latents = latents.half()\n",
        "\n",
        "      # set timesteps\n",
        "      accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "      extra_set_kwargs = {}\n",
        "      if accepts_offset:\n",
        "          extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "      if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "          latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "      # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "      # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "      # and should be between [0, 1]\n",
        "      accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "      extra_step_kwargs = {}\n",
        "      if accepts_eta:\n",
        "          extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "      for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "          # expand the latents if we are doing classifier free guidance\n",
        "          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              sigma = self.scheduler.sigmas[i]\n",
        "              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "          # predict the noise residual\n",
        "          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "          # perform guidance\n",
        "          if do_classifier_free_guidance:\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "          # compute the previous noisy sample x_t -> x_t-1\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "          else:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "      # scale and decode the image latents with vae\n",
        "      latents = 1 / 0.18215 * latents\n",
        "      image = self.vae.decode(latents)\n",
        "\n",
        "      image = (image / 2 + 0.5).clamp(0, 1)\n",
        "      image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "      # run safety checker\n",
        "      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "      safety_cheker_input.pixel_values = safety_cheker_input.pixel_values.half()\n",
        "      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "      if output_type == \"pil\":\n",
        "          image = self.numpy_to_pil(image)\n",
        "\n",
        "      return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}\n",
        "  ''')\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    del pipe.vae.encoder\n",
        "  else:\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "\n",
        "  # Noodle Soup prompts\n",
        "  nsp = False\n",
        "  use_this_prompt = False\n",
        "  try:\n",
        "    print(\":hourglass_not_done: Installing Noodle Soup Prompts...\")\n",
        "    wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py', './')\n",
        "  except ImportError:\n",
        "    nsp = False\n",
        "    raise ImportError('Unable to import Noodle Soup Prompts!')\n",
        "  finally:\n",
        "    import nsp_pantry\n",
        "    from nsp_pantry import nspterminology, nsp_parse\n",
        "\n",
        "  if nsp_parse and nspterminology:\n",
        "    print(\"\\r\\r:check_mark_button: \\33[32mNSP installed successfuly.\\33[0m \\x1B[3mMmm... Noodle Soup.\\x1B[0m\\n\")\n",
        "    nsp = True\n",
        "\n",
        "except OSError as e:\n",
        "  raise e\n",
        "except BaseException as e:\n",
        "  raise e\n",
        "finally:\n",
        "  if CLEAR_SETUP_LOG: clear()\n",
        "  print(f\"\\n--[ :confetti_ball::party_popper: \\033[1m\\33[32mEasy Diffusion Environtment Setup Complete\\33[0m :party_popper::confetti_ball: ]--\")\n",
        "\n",
        "from PIL import Image\n",
        "import random, time, shutil, sys, pprint\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from torch import autocast\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "Ucr5_i21xSjv"
      },
      "outputs": [],
      "source": [
        "#@title Render Images\n",
        "clean_env()\n",
        "#@markdown Prompts support [Noodle Soup Prompts](https://github.com/WASasquatch/noodle-soup-prompts/wiki/Terminology-Reference) \\([NSP Prompt Generator](https://rebrand.ly/noodle-soup-prompts)\\)\n",
        "PROMPT = \"A stylish _noun-emote_ cat in a _color_ space helmet on the moon\" #@param {type:'string'}\n",
        "PROMPT_FILE = '' #@param {type: 'string'}\n",
        "#@markdown `PROMPT_FILE` is a optional text file that contains a prompt per line. \n",
        "NEW_NSP_ON_ITERATION = True #@param{type: 'boolean'}\n",
        "#@markdown Wether to generate NSP once, or on each iteration. Check this if you want each iteration to have a freshly cooked noodle prompt.\n",
        "STEPS = 145 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "#@markdown Diffusion steps determines the quality of the final image\n",
        "SEED = 0 #@param {type:'integer'}\n",
        "#@markdown The seed used for the generation. Leave at `0` for random.\n",
        "NUM_ITERS = 5 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "#@markdown Number of iterations for a given prompt.\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 768 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "#@markdown The CFG scale determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "#@markdown If you're using the `LOW_VRAM_PATCH` you <b>must</b> use `autocast`<br>\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type: 'string'}\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "#@markdown `IMAGE_UPSCALER`: may not work at resolutions above 512x640/640x512 on GPUs with ~16GB VRAM.<br>**Note:** GFPGAN is good for faces only, and can create visual artifacts if the subject doesn't fill the frame\n",
        "IMAGE_UPSCALER = \"Both\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"Both\"]\n",
        "UPSCALE_AMOUNT = 2 #@param {type:\"raw\"}\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "%cd /content/\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "\n",
        "if IMAGE_UPSCALER == \"Both\" or IMAGE_UPSCALER == \"GFPGAN\":\n",
        "  if not os.path.exists('/content/GFPGAN'):\n",
        "    !git clone https://github.com/TencentARC/GFPGAN.git\n",
        "    %cd GFPGAN\n",
        "    # Set up the environment\n",
        "    # Install basicsr - https://github.com/xinntao/BasicSR\n",
        "    # We use BasicSR for both training and inference\n",
        "    !pip install basicsr\n",
        "    # Install facexlib - https://github.com/xinntao/facexlib\n",
        "    # We use face detection and face restoration helper in the facexlib package\n",
        "    !pip install facexlib\n",
        "    # Install other depencencies\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    !pip install realesrgan  # used for enhancing the background (non-face) regions\n",
        "    # Download the pre-trained model\n",
        "    # !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models\n",
        "    # Now we use the V1.3 model for the demo\n",
        "    !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "    %cd /content/\n",
        "    clear_output()\n",
        "    \n",
        "if IMAGE_UPSCALER == \"Both\" or IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "  def upscale(image):\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "    %cd Real-ESRGAN\n",
        "    from realesrgan import RealESRGAN\n",
        "    clear_output()\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    if not os.path.exists(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):\n",
        "      def closest_value(input_list, input_value):\n",
        "        difference = lambda input_list : abs(input_list - input_value)\n",
        "        res = min(input_list, key=difference)\n",
        "        return res\n",
        "      nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "      print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "      UPSCALE_AMOUNT = nearest_value\n",
        "\n",
        "    model = RealESRGAN(device, scale = UPSCALE_AMOUNT)\n",
        "    model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "    %cd ..\n",
        "\n",
        "# Diffuse Function\n",
        "def diffuse_run():\n",
        "\n",
        "    global SEED\n",
        "\n",
        "    if ORIG_SEED == 0:\n",
        "      if iteration is 0:\n",
        "        SEED = random.randint(0,sys.maxsize)\n",
        "      if iteration is not 0:\n",
        "        SEED += 1\n",
        "    else:\n",
        "      if iteration > 0:\n",
        "        SEED += 1\n",
        "    gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}, PROMPT:')\n",
        "    print(PROMPT)\n",
        "    \n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "    display(image)\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "    if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "      print('Face Enhancing and Upscaling... ')\n",
        "      %cd GFPGAN\n",
        "      !python inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s $UPSCALE_AMOUNT --bg_upsampler realesrgan\n",
        "      display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      print(f'Moving enhanced image to {OUTDIR}')\n",
        "      shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(image)\n",
        "      display(sr_image)\n",
        "      try:\n",
        "        sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      except NameError:\n",
        "        sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"Both\":\n",
        "      print('Face Enhancing... ')\n",
        "      %cd GFPGAN\n",
        "      !python inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s 1 --bg_upsampler realesrgan\n",
        "      display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      display(sr_image)\n",
        "      try:\n",
        "        sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      except NameError:\n",
        "        sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      clean_env()\n",
        "# End Diffuse Function\n",
        "\n",
        "if PROMPT.lower() in [None, '', 'none'] and PROMPT_FILE in [None, '', 'none']:\n",
        "    raise Exception(\"PROMPT and PROMPT_FILE are empty! You need to provide a PROMPT or PROMPT_FILE!\")\n",
        "\n",
        "PROMPTS = []\n",
        "if PROMPT_FILE not in ['','none']:\n",
        "    try:\n",
        "        with open(PROMPT_FILE, \"r\") as f:\n",
        "            PROMPTS = f.read().splitlines()\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "\n",
        "if PROMPT not in ['', 'none']:\n",
        "    PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "with precision_scope(\"cuda\"):\n",
        "\n",
        "    for pi in PROMPTS:\n",
        "\n",
        "        # Define Run Prompt\n",
        "        if NEW_NSP_ON_ITERATION is not True:\n",
        "            PROMPT = nsp_parse(pi)\n",
        "            epoch_time = int(time.time())\n",
        "            if SAVE_PROMPT_DETAILS:\n",
        "                with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "\n",
        "        for iteration in range(NUM_ITERS):\n",
        "\n",
        "            if NEW_NSP_ON_ITERATION:\n",
        "                PROMPT = nsp_parse(pi)\n",
        "                if SAVE_PROMPT_DETAILS:\n",
        "                    with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                            file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "\n",
        "            try:\n",
        "                diffuse_run()\n",
        "            except RuntimeError as e:\n",
        "                if 'out of memory' in str(e):\n",
        "                    print(f\"\\u001b[31m\\u001b[1m\\u001b[4mCRITICAL ERROR\\u001b[0m: {gpu_name} ran out of memory! Flushing memory to save session...\")\n",
        "                    pass\n",
        "                else:\n",
        "                    raise e\n",
        "            except KeyboardInterrupt as e:\n",
        "                print('\\nSeed used for this exited run:', SEED)\n",
        "                raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "            finally:\n",
        "                print(\"Cleaning up...\\n\")\n",
        "                clean_env()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Stability.AI Easy Diffusion",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}