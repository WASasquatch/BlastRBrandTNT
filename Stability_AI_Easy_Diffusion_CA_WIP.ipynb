{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WASasquatch/BlastRBrandTNT/blob/master/Stability_AI_Easy_Diffusion_CA_WIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6RXjS1tTji"
      },
      "source": [
        "# Stability.AI Easy Diffusion v0.5 ![visitors](https://visitor-badge.glitch.me/badge?page_id=EasyDiffusion&left_color=blue&right_color=orange) [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/WASasquatch/easydiffusion)\n",
        "\n",
        "A fork of NOP's Stable Diffusion Colab \n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G76cGaiuGdjJ"
      },
      "source": [
        "## Information\n",
        "\n",
        "Changelog:\n",
        "- v0.1: Forked [NOP's Stable Diffusion Colab v0.23](https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17?usp=sharing)\n",
        "  - Added File Prompts\n",
        "  - Added Noodle Soup Prompts\n",
        "- 8/25/2022) Added better image output display\n",
        "- 8/26/2022) Added `INIT_IMAGE` support\n",
        "  - Added basic image output option\n",
        "- 8/27/2022) Patched CodeFormer fidelity path bug\n",
        "- 8/27/2022) Various code tweaks (by plambe#5832)\n",
        "  - Download some of the dependencies to google drive if enabled \n",
        "    - For instance the stable diffusion model\n",
        "    - Also multiple of the git repos\n",
        "  - Replaced all `!` and `%` in code to make it more universal\n",
        "- 8/27/2022) Patch NSP Installation, changed paths for Stable Diffusion and output images. (by WAS#0263)\n",
        "- 8/27/2022) Organized and improved installations\n",
        "- 8/28/2022) Real-ESRGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) GFPGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) CodeFormer bug fix (by plambe#5832)\n",
        "- 8/28/2022) Added cached diffusion piping: This will speed up run performance, and save VRAM (WAS#0263)\n",
        "  - Added `RECACHE_PIPES` option\n",
        "  - Added `INCREMENT_ITERATION_SEED` option\n",
        "  - Patched working directory path for non-gdrive installations\n",
        "  - Patched working directory path for pipe cache not found\n",
        "  - Patched CodeFormer Fidelity paths, again?\n",
        "  - Added pre-ESRGAN down scaling option for GFPGAN + Real-ESRGAN, and CodeFormer + Real-ESRGAN.\n",
        "  - Added post diffusion sharpen option\n",
        "\n",
        "<br>\n",
        "\n",
        "## Stablity.AI Model Terms of Use\n",
        "\n",
        "**By using this Notebook, you agree to the following Terms of Use, and license**\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "\n",
        "1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "\n",
        "2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "\n",
        "3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOEF-K5F5db"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ekR-LW6trWG"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU Status\n",
        "#@markdown Check the status of the allocated GPU\n",
        "import subprocess\n",
        "print(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "nvidiasmi_simple = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "gpu_name = nvidiasmi_simple.split(':')[1].split('(')[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8gV4-qRDn1b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <font size=\"5\" color=\"orange\">**Setup Environment**</font>\n",
        "\n",
        "# Import future print\n",
        "from __future__ import print_function\n",
        "try:\n",
        "    import __builtin__\n",
        "except ImportError:\n",
        "    import builtins as __builtin__\n",
        "\n",
        "# Emoticon fun!\n",
        "import subprocess\n",
        "try:\n",
        "    import emoji\n",
        "except ImportError:\n",
        "     multipip_res = subprocess.run(['pip', '-q', 'install', 'emoji'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "finally:\n",
        "    import emoji\n",
        "\n",
        "# Override Print Function\n",
        "def print(message, *args, **kwargs):\n",
        "    if 'defaultprint' in kwargs:\n",
        "        kwargs.pop('defaultprint')\n",
        "        return __builtin__.print(message, *args, **kwargs)\n",
        "    else:\n",
        "        return __builtin__.print(emoji.emojize(message), *args, **kwargs)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Google Drive Options\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "#@markdown Use Google Drive to store images and prompt information\n",
        "USE_DRIVE_FOR_LOCAL_COPIES = False #@param {type:\"boolean\"}\n",
        "#@markdown Use Google Drive to store local copies of git repos, models and other assets<br>\n",
        "#@markdown <font color=\"orange\">**WARNING:**</font> Requires 14gb+ of space (not including images produced). May not be suitable for Free Google Drive accounts.<br>\n",
        "#@markdown &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you encounter issues loading pipes, or Upscalers, you're likely out of storage space. \n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Install Enhancement and Upscaling\n",
        "INSTALL_GFPGAN = True #@param{type:'boolean'}\n",
        "#@markdown Install GFPGAN Face Enhancement\n",
        "INSTALL_CODEFORMER = True #@param{type:'boolean'}\n",
        "#@markdown Install CodeFormer Face Enhancement\n",
        "INSTALL_ESRGAN = True #@param{type:'boolean'}\n",
        "#@markdown Install Real-ESRGAN Super Resolution\n",
        "INSTALL_KROMO = True #@param{type:'boolean'}\n",
        "#@markdown Install Kromo Chromatic Aberration gnerator\n",
        "\n",
        "#@markdown ---\n",
        "LOW_VRAM_PATCH = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"4\">`LOW_VRAM_PATCH`: Use 16bit float instead of 32bit float. This saves VRAM, at the potential cost of model fidelity.<br>**Note:** You may need this if you're using a GPU with ~16GB VRAM.</font><br>\n",
        "#@markdown ---\n",
        "ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "#@markdown `ENABLE_NSFW_FILTER`: Enable NSFW censoring<br>\n",
        "\n",
        "#@markdown ---\n",
        "CACHE_PIPELINES = False #@param{type: 'boolean'}\n",
        "#@markdown Whether to cache pipes to disk and load on demand (this can speed up diffusion start time)\n",
        "RECACHE_PIPES = True #@param{type: 'boolean'}\n",
        "#@markdown **NOTE:** If you're having trouble loading pipes to start diffusions, check this and run this cell again.<br>\n",
        "\n",
        "#@markdown ---\n",
        "CLEAR_SETUP_LOG = True #@param{type: 'boolean'}\n",
        "#@markdown Clear the setup log after installation compeltes.\n",
        "SUPPRESS_WARNINGS = True #@param{type: 'boolean'}\n",
        "#@markdown Supress warnings from installation scripts and runtime scripts.\n",
        "\n",
        "# Enable third-party widgets\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# SETUP BASE DIRECTORIES\n",
        "OUTDIR = '/content/Stable_Diffusion/images_out'\n",
        "\n",
        "import os, sys\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "STABLE_DIFFUSION_WORKDIR = '/content/Stable_Diffusion'\n",
        "GDRIVE_WORKDIR = '/content/drive/MyDrive/AI/Stable_Diffusion'\n",
        "\n",
        "if USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    STABLE_DIFFUSION_WORKDIR = GDRIVE_WORKDIR\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "if not USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "sys.path.append(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "# SETUP DEPENDENCIES\n",
        "\n",
        "import torch, gc, requests, io\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', '-q', '--show-progress', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def clean_env():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "clean_env()\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def clear():\n",
        "    from IPython.display import clear_output; return clear_output()\n",
        "\n",
        "# Basic image display\n",
        "def displayJsImage(i, name, img):\n",
        "    import cv2\n",
        "    from IPython.display import display, Javascript, clear_output\n",
        "    from google.colab.output import eval_js\n",
        "    from base64 import b64encode\n",
        "    from google.colab import files\n",
        "    img = np.asarray(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    js = Javascript('''\n",
        "        async function showImage(i, name, image, width, height) {\n",
        "        block = document.getElementById('block-'+i)\n",
        "        img = document.getElementById(name);\n",
        "        cont = document.getElementById(name+'_container');\n",
        "        if (block == null) {\n",
        "            block = document.createElement('div');\n",
        "            block.id = 'block-'+i;\n",
        "            block.style = 'width: auto;margin-bottom:15px;padding:5px;text-align:center;';\n",
        "            block.innerHTML = '<h3 style=\"margin:3px;text-align:center;\">Iteration '+i+'</h3>';\n",
        "            document.body.appendChild(block);\n",
        "        }\n",
        "        if(img == null && cont == null) {\n",
        "            cont = document.createElement('div');\n",
        "            cont.id = name+'_container';\n",
        "            link = document.createElement('a');\n",
        "            link.href = image;\n",
        "            link.target = '_blank';\n",
        "            img = document.createElement('img');\n",
        "            img.id = name;\n",
        "            img.class = \"resultImage\"\n",
        "            cont.style = 'display:inline-block;width:auto;font-size:14px;font-weight:bold;background-color:rgba(0,0,0,0.1);border-radius:5px;padding:2px;margin:2px;'\n",
        "            cont.innerHTML = '<p style=\"margin:3px auto;width:180px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;\">'+name+'</p>';\n",
        "            block.appendChild(cont);\n",
        "            cont.appendChild(link);\n",
        "            link.appendChild(img);\n",
        "        }\n",
        "        img.src = image;\n",
        "        img.style = \"margin: 5px; vertical-align: text-top; max-width: 256px; max-height: 512px;\";\n",
        "        }\n",
        "    ''')\n",
        "    height, width = img.shape[:2]\n",
        "    ret, data = cv2.imencode('.png', img)\n",
        "    data = b64encode(data)\n",
        "    data = data.decode()\n",
        "    data = 'data:image/png;base64,' + data\n",
        "    display(js)\n",
        "    eval_js(f'showImage({i}, \"{name}\", \"{data}\", {width}, {height})')\n",
        "\n",
        "def printPrompt(prompt, limit=12):\n",
        "    pw = prompt.split(\" \"); i=0; oi=0; pstr = ''\n",
        "    for w in pw:\n",
        "        oi+=1; pstr += f'{w} '\n",
        "        if i is limit or oi is len(pw): print(pstr.strip()); pstr = ''; i = 0; pass\n",
        "        i+=1\n",
        "\n",
        "def sharpenImage(image, samples=1):\n",
        "    import PIL\n",
        "    from PIL import Image, ImageFilter\n",
        "    im = image\n",
        "    for i in range(samples):\n",
        "        im = im.filter(ImageFilter.SHARPEN)\n",
        "    return im\n",
        "\n",
        "print(\"\\nStarting Installation Processess.\\nThis should take approximately one eternity...\\n\")\n",
        "\n",
        "try:\n",
        "  with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "    key = f.read().decode('utf-8').split(':')\n",
        "except OSError as e:\n",
        "  print(e)\n",
        "\n",
        "huggingface_username = key[0].strip()\n",
        "huggingface_token = key[1].strip()\n",
        "\n",
        "model_cache = f'{STABLE_DIFFUSION_WORKDIR}/model_cache'\n",
        "if not os.path.exists(model_cache):\n",
        "  os.makedirs(model_cache)\n",
        "\n",
        "if not os.path.exists(f'{model_cache}/sd-v1-4.ckpt'):\n",
        "  # Download model file\n",
        "  wget(f'https://{huggingface_username}:{huggingface_token}@huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt', f'{model_cache}')\n",
        "\n",
        "try:\n",
        "\n",
        "    # Install Joblib\n",
        "    try:\n",
        "        import joblib\n",
        "        from joblib import Memory\n",
        "    except ImportError:\n",
        "        res = subprocess.run(['pip', 'install', 'joblib'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        print(res)\n",
        "    finally:\n",
        "        import joblib\n",
        "        from joblib import Memory\n",
        "        cache_dir = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "\n",
        "    # Install Shutup\n",
        "    try:\n",
        "        import shutup; \n",
        "    except ImportError:\n",
        "        res = subprocess.run(['pip', 'install', 'shutup'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        print(res)\n",
        "    finally:    \n",
        "        import shutup; \n",
        "        if SUPPRESS_WARNINGS: \n",
        "            shutup.please()\n",
        "\n",
        "    import warnings\n",
        "    if SUPPRESS_WARNINGS:\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "    \n",
        "    res = subprocess.run(['git', 'lfs', 'install'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    os.environ['GIT_LFS_SKIP_SMUDGE'] = \"0\"\n",
        "    # This will take a while\n",
        "\n",
        "    res = subprocess.run(['git', 'lfs', 'clone', 'https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['pip', 'install', '-U', 'git+https://github.com/huggingface/diffusers.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    # Disable NSFW check\n",
        "    if not ENABLE_NSFW_FILTER:\n",
        "        with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:\n",
        "            file.write('''\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\n",
        "\n",
        "from ...utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "def cosine_distance(image_embeds, text_embeds):\n",
        "    normalized_image_embeds = nn.functional.normalize(image_embeds)\n",
        "    normalized_text_embeds = nn.functional.normalize(text_embeds)\n",
        "    return torch.mm(normalized_image_embeds, normalized_text_embeds.T)\n",
        "\n",
        "\n",
        "class StableDiffusionSafetyChecker(PreTrainedModel):\n",
        "    config_class = CLIPConfig\n",
        "\n",
        "    def __init__(self, config: CLIPConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.vision_model = CLIPVisionModel(config.vision_config)\n",
        "        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n",
        "\n",
        "        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n",
        "        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n",
        "\n",
        "        self.register_buffer(\"concept_embeds_weights\", torch.ones(17))\n",
        "        self.register_buffer(\"special_care_embeds_weights\", torch.ones(3))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, clip_input, images):\n",
        "        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n",
        "        image_embeds = self.visual_projection(pooled_output)\n",
        "\n",
        "        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().numpy()\n",
        "        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().numpy()\n",
        "\n",
        "        result = []\n",
        "        batch_size = image_embeds.shape[0]\n",
        "        for i in range(batch_size):\n",
        "            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n",
        "\n",
        "            # increase this value to create a stronger `nfsw` filter\n",
        "            # at the cost of increasing the possibility of filtering benign images\n",
        "            adjustment = 0.0\n",
        "\n",
        "            for concet_idx in range(len(special_cos_dist[0])):\n",
        "                concept_cos = special_cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.special_care_embeds_weights[concet_idx].item()\n",
        "                result_img[\"special_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"special_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"special_care\"].append({concet_idx, result_img[\"special_scores\"][concet_idx]})\n",
        "                    adjustment = 0.01\n",
        "\n",
        "            for concet_idx in range(len(cos_dist[0])):\n",
        "                concept_cos = cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.concept_embeds_weights[concet_idx].item()\n",
        "                result_img[\"concept_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"concept_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"bad_concepts\"].append(concet_idx)\n",
        "\n",
        "            result.append(result_img)\n",
        "\n",
        "        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n",
        "\n",
        "        #for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n",
        "        #    if has_nsfw_concept:\n",
        "        #        images[idx] = np.zeros(images[idx].shape)  # black image\n",
        "\n",
        "        if any(has_nsfw_concepts):\n",
        "            logger.warning(\n",
        "                \"Potential NSFW content was detected in one or more images, but the NSFW filter is off.\"\n",
        "            )\n",
        "\n",
        "        return images, has_nsfw_concepts''')\n",
        "\n",
        "    res = subprocess.run(['pip', 'install', 'transformers'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    # make sure you're logged in with `huggingface-cli login`\n",
        "    from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "\n",
        "    # lms = LMSDiscreteScheduler(\n",
        "    #     beta_start=0.00085, \n",
        "    #     beta_end=0.012, \n",
        "    #     beta_schedule=\"scaled_linear\"\n",
        "    # )\n",
        "\n",
        "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "    if not os.path.exists('diffusers_output'):\n",
        "        os.makedirs('diffusers_output')\n",
        "\n",
        "    res = subprocess.run(['pip', 'install',\n",
        "                            'pytorch-pretrained-bert',\n",
        "                            'spacy',\n",
        "                            'ftfy',\n",
        "                            ], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['python', '-m', 'spacy', 'download', 'en'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['pip', 'install', 'scipy'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['git', 'clone', '--recursive', 'https://github.com/crowsonkb/k-diffusion.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    left_of_pipe = subprocess.Popen([\"echo\", huggingface_token], stdout=subprocess.PIPE)\n",
        "    right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(right_of_pipe)\n",
        "\n",
        "    if LOW_VRAM_PATCH:\n",
        "        patched_file = open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py').read().replace('input, self.num_groups, self.weight, self.bias, self.eps)','input, self.num_groups, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)')\n",
        "        with open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py','w') as file:\n",
        "            file.write(patched_file)\n",
        "        with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py','w') as file:\n",
        "            file.write(\n",
        "  '''\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from ...models import AutoencoderKL, UNet2DConditionModel\n",
        "from ...pipeline_utils import DiffusionPipeline\n",
        "from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from .safety_checker import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vae: AutoencoderKL,\n",
        "      text_encoder: CLIPTextModel,\n",
        "      tokenizer: CLIPTokenizer,\n",
        "      unet: UNet2DConditionModel,\n",
        "      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "      safety_checker: StableDiffusionSafetyChecker,\n",
        "      feature_extractor: CLIPFeatureExtractor,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      scheduler = scheduler.set_format(\"pt\")\n",
        "      self.register_modules(\n",
        "          vae=vae,\n",
        "          text_encoder=text_encoder,\n",
        "          tokenizer=tokenizer,\n",
        "          unet=unet,\n",
        "          scheduler=scheduler,\n",
        "          safety_checker=safety_checker,\n",
        "          feature_extractor=feature_extractor,\n",
        "      )\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(\n",
        "      self,\n",
        "      prompt: Union[str, List[str]],\n",
        "      height: Optional[int] = 512,\n",
        "      width: Optional[int] = 512,\n",
        "      num_inference_steps: Optional[int] = 50,\n",
        "      guidance_scale: Optional[float] = 7.5,\n",
        "      eta: Optional[float] = 0.0,\n",
        "      generator: Optional[torch.Generator] = None,\n",
        "      output_type: Optional[str] = \"pil\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "      if \"torch_device\" in kwargs:\n",
        "          device = kwargs.pop(\"torch_device\")\n",
        "          warnings.warn(\n",
        "              \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "              \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "          )\n",
        "\n",
        "          # Set device as before (to be removed in 0.3.0)\n",
        "          if device is None:\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "          self.to(device)\n",
        "\n",
        "      if isinstance(prompt, str):\n",
        "          batch_size = 1\n",
        "      elif isinstance(prompt, list):\n",
        "          batch_size = len(prompt)\n",
        "      else:\n",
        "          raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "      if height % 8 != 0 or width % 8 != 0:\n",
        "          raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "      # get prompt text embeddings\n",
        "      text_input = self.tokenizer(\n",
        "          prompt,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "      # corresponds to doing no classifier free guidance.\n",
        "      do_classifier_free_guidance = guidance_scale > 1.0\n",
        "      # get unconditional embeddings for classifier free guidance\n",
        "      if do_classifier_free_guidance:\n",
        "          max_length = text_input.input_ids.shape[-1]\n",
        "          uncond_input = self.tokenizer(\n",
        "              [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "          )\n",
        "          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "          # For classifier free guidance, we need to do two forward passes.\n",
        "          # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "          # to avoid doing two forward passes\n",
        "          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      # get the intial random noise\n",
        "      latents = torch.randn(\n",
        "          (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "          generator=generator,\n",
        "          device=self.device,\n",
        "      )\n",
        "      latents = latents.half()\n",
        "\n",
        "      # set timesteps\n",
        "      accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "      extra_set_kwargs = {}\n",
        "      if accepts_offset:\n",
        "          extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "      if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "          latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "      # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "      # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "      # and should be between [0, 1]\n",
        "      accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "      extra_step_kwargs = {}\n",
        "      if accepts_eta:\n",
        "          extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "      steps_bar = tqdm(range(num_inference_steps), desc='Steps')\n",
        "      #for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "      for i, t in enumerate(self.scheduler.timesteps):\n",
        "          steps_bar.n = i\n",
        "          steps_bar.refresh()\n",
        "          # expand the latents if we are doing classifier free guidance\n",
        "          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              sigma = self.scheduler.sigmas[i]\n",
        "              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "          # predict the noise residual\n",
        "          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "          # perform guidance\n",
        "          if do_classifier_free_guidance:\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "          # compute the previous noisy sample x_t -> x_t-1\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "          else:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "      # scale and decode the image latents with vae\n",
        "      latents = 1 / 0.18215 * latents\n",
        "      image = self.vae.decode(latents)\n",
        "\n",
        "      image = (image / 2 + 0.5).clamp(0, 1)\n",
        "      image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "      # run safety checker\n",
        "      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "      if output_type == \"pil\":\n",
        "          image = self.numpy_to_pil(image)\n",
        "\n",
        "      return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}\n",
        "  ''')\n",
        "\n",
        "    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py', 'w') as file:\n",
        "        file.write(\n",
        "  '''\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from ...models import AutoencoderKL, UNet2DConditionModel\n",
        "from ...pipeline_utils import DiffusionPipeline\n",
        "from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from .safety_checker import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vae: AutoencoderKL,\n",
        "      text_encoder: CLIPTextModel,\n",
        "      tokenizer: CLIPTokenizer,\n",
        "      unet: UNet2DConditionModel,\n",
        "      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "      safety_checker: StableDiffusionSafetyChecker,\n",
        "      feature_extractor: CLIPFeatureExtractor,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      scheduler = scheduler.set_format(\"pt\")\n",
        "      self.register_modules(\n",
        "          vae=vae,\n",
        "          text_encoder=text_encoder,\n",
        "          tokenizer=tokenizer,\n",
        "          unet=unet,\n",
        "          scheduler=scheduler,\n",
        "          safety_checker=safety_checker,\n",
        "          feature_extractor=feature_extractor,\n",
        "      )\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(\n",
        "      self,\n",
        "      prompt: Union[str, List[str]],\n",
        "      height: Optional[int] = 512,\n",
        "      width: Optional[int] = 512,\n",
        "      num_inference_steps: Optional[int] = 50,\n",
        "      guidance_scale: Optional[float] = 7.5,\n",
        "      eta: Optional[float] = 0.0,\n",
        "      generator: Optional[torch.Generator] = None,\n",
        "      output_type: Optional[str] = \"pil\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "      if \"torch_device\" in kwargs:\n",
        "          device = kwargs.pop(\"torch_device\")\n",
        "          warnings.warn(\n",
        "              \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "              \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "          )\n",
        "\n",
        "          # Set device as before (to be removed in 0.3.0)\n",
        "          if device is None:\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "          self.to(device)\n",
        "\n",
        "      if isinstance(prompt, str):\n",
        "          batch_size = 1\n",
        "      elif isinstance(prompt, list):\n",
        "          batch_size = len(prompt)\n",
        "      else:\n",
        "          raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "      if height % 8 != 0 or width % 8 != 0:\n",
        "          raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "      # get prompt text embeddings\n",
        "      text_input = self.tokenizer(\n",
        "          prompt,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "      # corresponds to doing no classifier free guidance.\n",
        "      do_classifier_free_guidance = guidance_scale > 1.0\n",
        "      # get unconditional embeddings for classifier free guidance\n",
        "      if do_classifier_free_guidance:\n",
        "          max_length = text_input.input_ids.shape[-1]\n",
        "          uncond_input = self.tokenizer(\n",
        "              [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "          )\n",
        "          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "          # For classifier free guidance, we need to do two forward passes.\n",
        "          # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "          # to avoid doing two forward passes\n",
        "          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      # get the intial random noise\n",
        "      latents = torch.randn(\n",
        "          (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "          generator=generator,\n",
        "          device=self.device,\n",
        "      )\n",
        "      latents = latents.half()\n",
        "\n",
        "      # set timesteps\n",
        "      accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "      extra_set_kwargs = {}\n",
        "      if accepts_offset:\n",
        "          extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "      if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "          latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "      # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "      # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "      # and should be between [0, 1]\n",
        "      accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "      extra_step_kwargs = {}\n",
        "      if accepts_eta:\n",
        "          extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "      steps_bar = tqdm(range(num_inference_steps), desc='Steps')\n",
        "      #for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "      for i, t in enumerate(self.scheduler.timesteps):\n",
        "          steps_bar.n = i\n",
        "          steps_bar.refresh()\n",
        "          # expand the latents if we are doing classifier free guidance\n",
        "          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              sigma = self.scheduler.sigmas[i]\n",
        "              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "          # predict the noise residual\n",
        "          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "          # perform guidance\n",
        "          if do_classifier_free_guidance:\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "          # compute the previous noisy sample x_t -> x_t-1\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "          else:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "      # scale and decode the image latents with vae\n",
        "      latents = 1 / 0.18215 * latents\n",
        "      image = self.vae.decode(latents)\n",
        "\n",
        "      image = (image / 2 + 0.5).clamp(0, 1)\n",
        "      image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "      # run safety checker\n",
        "      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "      safety_cheker_input.pixel_values = safety_cheker_input.pixel_values.half()\n",
        "      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "      if output_type == \"pil\":\n",
        "          image = self.numpy_to_pil(image)\n",
        "\n",
        "      return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}\n",
        "  ''')\n",
        "        \n",
        "    # Image-to-Image\n",
        "    if not os.path.exists('image_to_image.py'):\n",
        "        wget('https://raw.githubusercontent.com/huggingface/diffusers/4674fdf807cdefd4db1758067c0207872d805f8c/examples/inference/image_to_image.py', './')\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from image_to_image import StableDiffusionImg2ImgPipeline, preprocess\n",
        "        \n",
        "    # Setup Piping Cache\n",
        "    if CACHE_PIPELINES:\n",
        "        print('\\n:gear: Setting up Stable Diffusion Pipeline...')\n",
        "        model_cache = f'{STABLE_DIFFUSION_WORKDIR}/model_cache'\n",
        "        pipe_cache = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "        if not os.path.exists(model_cache):\n",
        "            os.makedirs(model_cache)\n",
        "\n",
        "        if not os.path.exists(pipe_cache):\n",
        "            os.makedirs(pipe_cache)\n",
        "\n",
        "        # DUMP PIPING\n",
        "        clean_env()\n",
        "        if LOW_VRAM_PATCH:\n",
        "            if not os.path.exists(f'{pipe_cache}/LOW_VRAM_PIPE.obj') or RECACHE_PIPES:\n",
        "                joblib.dump(StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/LOW_VRAM_PIPE.obj')\n",
        "                #del piping['LOW_VRAM'].vae.encoder\n",
        "                clean_env()\n",
        "        if not os.path.exists(f'{pipe_cache}/IMG2IMG_PIPE.obj') or RECACHE_PIPES:\n",
        "            joblib.dump(StableDiffusionImg2ImgPipeline.from_pretrained(model_id, cache_dir=model_cache, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/IMG2IMG_PIPE.obj')\n",
        "            clean_env()\n",
        "        if not os.path.exists(f'{pipe_cache}/DEFAULT.obj') or RECACHE_PIPES:\n",
        "            joblib.dump(StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/DEFAULT_PIPE.obj')\n",
        "            clean_env()\n",
        "        print(\":check_mark_button: Pipeline setup complete.\\n\")\n",
        "\n",
        "    if INSTALL_GFPGAN:\n",
        "        print(\"\\n:hourglass_not_done: Installing GFPGAN...\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN'):\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/TencentARC/GFPGAN.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            # Set up the environment\n",
        "            # used for enhancing the background (non-face) regions\n",
        "            # Download the pre-trained model\n",
        "            # Now we use the V1.3 model for the demo\n",
        "            wget(\"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\", \"experiments/pretrained_models\")\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            \n",
        "        # Install basicsr - https://github.com/xinntao/BasicSR\n",
        "        # We use BasicSR for both training and inference\n",
        "        # Install facexlib - https://github.com/xinntao/facexlib\n",
        "        # We use face detection and face restoration helper in the facexlib package\n",
        "        # Install other depencencies\n",
        "        print(subprocess.run(['pip', 'install', 'basicsr', 'facexlib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['python', 'setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['pip', 'install', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(\":check_mark_button: GFPGAN installed!\\n\")\n",
        "        \n",
        "    if INSTALL_ESRGAN:\n",
        "        print(\"\\n:hourglass_not_done: Installing Real-ESRGAN\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN'):\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            print(subprocess.run(['pip', 'install', '-r', 'Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth\", \"Real-ESRGAN/weights/\")\n",
        "        print(\":check_mark_button: Real-ESRGAN installed!\")\n",
        "        \n",
        "        def upscale(image, scale):\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            from realesrgan import RealESRGAN\n",
        "            device = torch.device('cuda')\n",
        "            model = RealESRGAN(device, scale = scale)\n",
        "            model.load_weights(f'weights/RealESRGAN_x{scale}.pth')\n",
        "            sr_image = model.predict(np.array(image))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "            return sr_image\n",
        "\n",
        "    if INSTALL_CODEFORMER:\n",
        "        print(\":hourglass_not_done: Installing CodeFormer...\\n\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/sczhou/CodeFormer.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "        print(subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        # Install basicsr\n",
        "        print(subprocess.run(['python', 'basicsr/setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "        # Download the pre-trained model\n",
        "        print(subprocess.run(['python', 'scripts/download_pretrained_models.py', 'facelib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['python', 'scripts/download_pretrained_models.py', 'CodeFormer'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.makedirs('temp', exist_ok=True)\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        print(\":check_mark_button: CodeFormer installed!\")\n",
        "\n",
        "    if INSTALL_KROMO:\n",
        "        print(\":hourglass_not_done: Installing Kromo...\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/kromo'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/yoonsikp/kromo'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "        print(subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "    # Noodle Soup prompts\n",
        "    try:\n",
        "        import nsp_pantry\n",
        "    except ImportError:\n",
        "        if not os.path.exists('nsp_pantry.py'):\n",
        "            print(\":hourglass_not_done: Installing Noodle Soup Prompts...\")\n",
        "            wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py', './')\n",
        "    finally:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nspterminology, nsp_parse\n",
        "\n",
        "    if nsp_parse and nspterminology:\n",
        "        print(\"\\r\\r:check_mark_button: \\33[32mNSP installed successfuly.\\33[0m \\x1B[3mMmm... Noodle Soup.\\x1B[0m\\n\")\n",
        "\n",
        "except OSError as e:\n",
        "    raise e\n",
        "except BaseException as e:\n",
        "    raise e\n",
        "finally:\n",
        "    if CLEAR_SETUP_LOG: clear()\n",
        "    print(f\"\\n--[ :confetti_ball::party_popper: \\033[1m\\33[32mEasy Diffusion Environtment Setup Complete\\33[0m :party_popper::confetti_ball: ]--\")\n",
        "\n",
        "from PIL import Image\n",
        "import random, time, shutil, sys, pprint\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from torch import autocast\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucr5_i21xSjv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <font size=\"5\" color=\"green\">**Settings & Diffuse**</font>\n",
        "clean_env()\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Prompt Setup**\n",
        "#@markdown Prompts support [Noodle Soup Prompts](https://github.com/WASasquatch/noodle-soup-prompts/wiki/Terminology-Reference) \\([NSP Prompt Generator](https://rebrand.ly/noodle-soup-prompts)\\)\n",
        "PROMPT = \"A photorealistic 3d scene of a massive capital spaceship in a hazey detailed nebula fireing giant lasers at hundreds of small fighter spaceships by Pengzhen Zhang and Pablo Dominguez, trending on ArtStation, featured on CGSocieity, in a _color_ color scheme, _gen-modifier_, _3d-terms_, _3d-terms_, _hd_, _hd_\" #@param {type:'string'}\n",
        "PROMPT_FILE = '' #@param {type: 'string'}\n",
        "#@markdown `PROMPT_FILE` is a optional text file that contains a prompt per line. \n",
        "NEW_NSP_ON_ITERATION = True #@param{type: 'boolean'}\n",
        "#@markdown Whether to generate NSP once, or on each iteration. Check this if you want each iteration to have a freshly cooked noodle prompt.\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Init Image Setup**\n",
        "INIT_IMAGE = \"\" #@param {type: 'string'}\n",
        "#@markdown A path or URL to a init image to use as a initial starting point for diffusion\n",
        "INIT_STRENGTH = 0.5 #@param{type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Diffusion Settings**\n",
        "STEPS = 145 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "#@markdown Diffusion steps determines the quality of the final image\n",
        "SEED = 0 #@param {type:'integer'}\n",
        "#@markdown The seed used for the generation. Leave at `0` for random.\n",
        "INCREMENT_ITERATION_SEED = True #@param{type: 'boolean'}\n",
        "#@markdown Disable this if you want a new random seed each iteration, or the same unique seed each iteration.\n",
        "NUM_ITERS = 20 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "#@markdown Number of iterations for a given prompt.\n",
        "WIDTH = 1024 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.5 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "#@markdown The CFG `SCALE` determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "#@markdown If you're using the `LOW_VRAM_PATCH` you <b>must</b> use `autocast`<br>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Upscaling Settings**\n",
        "#@markdown `IMAGE_UPSCALER`: may not work at resolutions above 512x768/768x512 on GPUs with ~16GB VRAM.<br>**Note:** GFPGAN is good for faces only, and can create visual artifacts if the subject doesn't fill the frame\n",
        "IMAGE_UPSCALER = \"Enhanced Real-ESRGAN\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\"]\n",
        "UPSCALE_AMOUNT = 2 #@param {type:\"slider\", min:2, max:8, step:2}\n",
        "CODEFORMER_FIDELITY = 0.25 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown `CODEFORMER_FIDELITY`: Only applies if the upscaler includes Codeformer. Balance the quality (lower number) and fidelity (higher number)<br>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Image Adjustments**\n",
        "SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN = True #@param{type:'boolean'}\n",
        "#@markdown Scale down enhanced images. Useful if you are also using Real-ESRGAN. This will preserve your upscale factor for Real-ESRGAN after GFPGAN or CodeFormer.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "SHARPEN_DIFFUSION_IMAGE = True #@param{type: 'boolean'}\n",
        "#@markdown Sharpen the base diffusion image before upscsaling.\n",
        "SHARPEN_AMOUNT = 1 #@param{type:'slider', min:1, max:3, step:1}\n",
        "#@markdown Sharpen iteration amount.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "CA_DIFFUSE_IMAGE = True #@param{type: 'boolean'}\n",
        "#@markdown Apply Chromatic Aberration to the base diffusion image (pre sharpen if enabled)\n",
        "CA_STRENGTH = 0.2 #@param {type:\"slider\", min:0, max:5, step:0.1}\n",
        "#@markdown Chromatic Aberration strength\n",
        "CA_JITTER = 2 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "#@markdown Chromatic Aberration set channel offset pixels\n",
        "CA_OVERLAY = 0.25 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown Alpha of original image overlay.\n",
        "CA_NO_RADIAL_BLUR = False #@param{type: 'boolean'}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Other Settings**\n",
        "IMAGES_FOLDER = \"time_to_stabilize\" #@param {type: 'string'}\n",
        "#@markdown Define a custom folder to saves images within your `images_out` folder. Example: `CAR_CONCEPTS`<br>\n",
        "#@markdown **Note:** Path: `/content/Stable_Diffusion/images_out` or with Google Drive `/content/drive/MyDrive/AI/Stable_Diffusion/images_out`\n",
        "USE_BASIC_IMAGE_DISPLAY = False #@param{type: 'boolean'}\n",
        "#@markdown Use basic image output instead of organized JS Image Output\n",
        "\n",
        "if LOW_VRAM_PATCH and PRECISION is not 'autocast': \n",
        "    print(f\"PRECISION must be 'autocast' when running in low vram compatibility mode! Defaulting to autocast...\")\n",
        "    PRECISION = 'autocast'\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "GDRIVE_OUT_PATH = f'{GDRIVE_WORKDIR}/images_out/{IMAGES_FOLDER}'\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "    if not os.path.exists(GDRIVE_OUT_PATH):\n",
        "        os.makedirs(GDRIVE_OUT_PATH)\n",
        "    OUTDIR = GDRIVE_OUT_PATH        \n",
        "\n",
        "print(f\"Images Output Directory: {OUTDIR}\\n\")\n",
        "\n",
        "# Check Upscaling Mode\n",
        "if IMAGE_UPSCALER == 'GFPGAN' and not INSTALL_GFPGAN:\n",
        "    print(\":WARNING: GFPGAN Face Restoration is not installed. Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'Enhanced Real-ESRGAN' and not INSTALL_ESRGAN:\n",
        "    print(\":WARNING: Real-ESRGAN is not installed. Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'CodeFormer' and not INSTALL_CODEFORMER:\n",
        "    print(\":WARNING: CodeFormer is not installed! Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'GFPGAN + Enhanced ESRGAN':\n",
        "    if not INSTALL_GFPGAN and INSTALL_ESRGAN:\n",
        "        print(\":WARNING: GFPGAN is not installed, defaulting to Real-ESRGAN...\")\n",
        "        IMAGE_UPSCALER = 'Enhanced Real-ESRGAN'\n",
        "    if not INSTALL_ESRGAN and INSTALL_GFPGAN:\n",
        "        print(\":WARNING: Real-ESRGAN is not installed, defaulting to GFPGAN...\")\n",
        "        IMAGE_UPSCALER = 'GFPGAN'\n",
        "if IMAGE_UPSCALER == 'CodeFormer + Enhanced ESRGAN':\n",
        "    if not INSTALL_CODEFORMER and INSTALL_ESRGAN:\n",
        "        print(\":WARNING: CodeFormer is not installed, defaulting to Real-ESRGAN...\")\n",
        "        IMAGE_UPSCALER = 'Enhanced Real-ESRGAN'\n",
        "    if not INSTALL_ESRGAN and INSTALL_CODEFORMER:\n",
        "        print(\":WARNING: Real-ESRGAN is not installed, defaulting to CodeFormer...\")\n",
        "        IMAGE_UPSCALER = 'CodeFormer'\n",
        "\n",
        "def closest_value(input_list, input_value):\n",
        "    difference = lambda input_list : abs(input_list - input_value)\n",
        "    res = min(input_list, key=difference)\n",
        "    return res\n",
        "\n",
        "nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "\n",
        "# Diffuse Function\n",
        "def diffuse_run():\n",
        "\n",
        "    clean_env()\n",
        "\n",
        "    global SEED, UPSCALE_AMOUNT\n",
        "    if not CACHE_PIPELINES: global pipe\n",
        "\n",
        "    if ORIG_SEED is 0 and SEED is 0:\n",
        "        SEED = random.randint(0,sys.maxsize)\n",
        "    else:\n",
        "        if INCREMENT_ITERATION_SEED and iteration > 0:\n",
        "            SEED += 1\n",
        "\n",
        "    gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    print(f\"\\n\\033[1mIteration {iteration}\\033[0m\")\n",
        "    print(f':seedling: Seed: \\033[1m{SEED}\\033[0m, :triangular_ruler: Scale: \\033[1m{SCALE}\\033[0m, :footprints: Steps: \\033[1m{STEPS}\\033[0m, :framed_picture: Resolution: \\033[1m{WIDTH} x {HEIGHT}\\033[0m, :black_nib: Prompt:')\n",
        "    print(\"\\033[1m\")\n",
        "    printPrompt(PROMPT)\n",
        "    print(\"\\033[0m\\n\")\n",
        "\n",
        "    if init is not None:\n",
        "      if USE_BASIC_IMAGE_DISPLAY:\n",
        "          print(\"Resized Init Image:\")\n",
        "          display(original_init)\n",
        "      else:\n",
        "          displayJsImage(iteration, f'Resized Init Image {iteration}', original_init)\n",
        "\n",
        "    if CACHE_PIPELINES:\n",
        "        clean_env()\n",
        "        print(':gear: Loading Stable Diffusion Pipeline from cache...')\n",
        "        if init is None and LOW_VRAM_PATCH:\n",
        "            pipe = joblib.load(f'{STABLE_DIFFUSION_WORKDIR}/cache/LOW_VRAM_PIPE.obj')\n",
        "            del pipe.vae.encoder\n",
        "            clean_env()\n",
        "        elif init is not None:\n",
        "            pipe = joblib.load(f'{STABLE_DIFFUSION_WORKDIR}/cache/IMG2IMG_PIPE.obj')\n",
        "            clean_env()\n",
        "        else:\n",
        "            pipe = joblib.load(f'{STABLE_DIFFUSION_WORKDIR}/cache/DEFAULT_PIPE.obj')\n",
        "            clean_env()\n",
        "        print(\":check_mark_button: Pipeline loaded.\\n\")\n",
        "\n",
        "    try:\n",
        "        print(f\":alembic: Starting Diffusion run with {model_id}\")\n",
        "        if init is not None:\n",
        "            with autocast(\"cuda\"):\n",
        "                image = pipe(prompt=PROMPT, init_image=init, strength=INIT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        else:\n",
        "            image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "    except BaseException as e:\n",
        "        raise e\n",
        "    finally:\n",
        "        print(\":check_mark_button: Diffusion Complete!\\n\")\n",
        "        clean_env()\n",
        "        if CACHE_PIPELINES:\n",
        "            del pipe\n",
        "\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "\n",
        "    if INSTALL_KROMO:\n",
        "        if CA_DIFFUSE_IMAGE:\n",
        "            print(f\"Applying chromatic aberration to result image.\\n\")\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "            ca_no_blur = '-n ' if CA_NO_RADIAL_BLUR else ''\n",
        "            print(subprocess.run(f'python kromo.py -s {CA_STRENGTH} -j {CA_JITTER} -y {CA_OVERLAY} {ca_no_blur}-o {filedir} {filedir}'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            image = Image.open(filedir).resize((WIDTH,HEIGHT))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    else:\n",
        "        print(\"Kromo is not installed! Please check INSTALL_KROMO and run the environment setup again.\")\n",
        "\n",
        "    if SHARPEN_DIFFUSION_IMAGE:\n",
        "        print(f\"Shapening diffusion result with {SHARPEN_AMOUNT} passes.\\n\")\n",
        "        image = sharpenImage(image, SHARPEN_AMOUNT)\n",
        "\n",
        "    if USE_BASIC_IMAGE_DISPLAY:\n",
        "        display(image)\n",
        "    else:\n",
        "        displayJsImage(iteration, f'Stability Diffusion {iteration}', image)\n",
        "\n",
        "    if 'ESRGAN' in IMAGE_UPSCALER:\n",
        "        os.chdir(f\"{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN\")\n",
        "        if not os.path.exists(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    if INSTALL_GFPGAN:\n",
        "        if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "            clean_env()\n",
        "            print(':sparkle: GFPGAN Face Restoration... ')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s {UPSCALE_AMOUNT} --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "            else:\n",
        "                displayJsImage(iteration, f'GFPGAN {iteration}', Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            print(f'Moving enhanced image to {OUTDIR}')\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            clean_env()\n",
        "    else:\n",
        "        print(\"GFPGAN is not installed! Please check INSTALL_GFPGAN and run the environment setup again.\")\n",
        "\n",
        "    if INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "            clean_env()\n",
        "            print(':multiply: Real-ESRGAN Upscaling... ')\n",
        "            print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            UPSCALE_AMOUNT = nearest_value\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            sr_image = upscale(image, UPSCALE_AMOUNT)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage(iteration, f'Real-ESRGAN {iteration}', sr_image)\n",
        "            try:\n",
        "                sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            except NameError:\n",
        "                sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            clean_env()\n",
        "    else:\n",
        "        print(\"Real-ESRGAN is not installed! Please check INSTALL_ESRGAN and run the environment setup again.\")\n",
        "\n",
        "\n",
        "    if INSTALL_GFPGAN and INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"GFPGAN + Enhanced ESRGAN\":\n",
        "            clean_env()\n",
        "\n",
        "            # GFPGAN\n",
        "            print(':sparkle: GFPGAN Face Restoration... ')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s 1 --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            \n",
        "            # Real-ESRGAN\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            enhanced_image = Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "                enhanced_image = enhanced_image.resize((WIDTH,HEIGHT))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(enhanced_image)\n",
        "            else:\n",
        "                displayJsImage(iteration, f'GFPGAN {iteration}', enhanced_image)\n",
        "            print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "            if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "              UPSCALE_AMOUNT = nearest_value\n",
        "              print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            sr_image = upscale(enhanced_image, UPSCALE_AMOUNT)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage(iteration, f'Real-ESRGAN {iteration}', sr_image)\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            clean_env()\n",
        "    else:\n",
        "        if not INSTALL_GFPGAN:\n",
        "            print(\"GFPGAN is not installed! Please check INSTALL_GFPGAN and run the environment setup again.\")\n",
        "        if not INSTALL_ESRGAN:\n",
        "            print(\"Real-ESRGAN is not installed! Please check INSTALL_ESRGAN and run the environment setup again.\")\n",
        "\n",
        "    if INSTALL_CODEFORMER:\n",
        "        if IMAGE_UPSCALER == \"CodeFormer\":\n",
        "            clean_env()\n",
        "            print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "            # It was behaving weird, hence why I am doing this the weird way\n",
        "            print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "            print(subprocess.run(f'python inference_codeformer.py --w {CODEFORMER_FIDELITY} --test_path {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp --upscale {UPSCALE_AMOUNT} --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "            shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/temp_{float(CODEFORMER_FIDELITY)}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "            else:\n",
        "                displayJsImage(iteration, f'CodeFormer {iteration}', Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "            clean_env()\n",
        "    else:\n",
        "        print(\"CodeFormer is not installed! Please check CodeFormer and run the environment setup again.\")\n",
        "\n",
        "\n",
        "    if INSTALL_CODEFORMER and INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"CodeFormer + Enhanced ESRGAN\":\n",
        "            clean_env()\n",
        "            # CodeFormer\n",
        "            print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "            print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "            print(subprocess.run(f'python inference_codeformer.py --w {CODEFORMER_FIDELITY} --test_path {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "            shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/temp_{float(CODEFORMER_FIDELITY)}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            \n",
        "            # Real-ESRGAN\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            enhanced_image = Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "                enhanced_image = enhanced_image.resize((WIDTH,HEIGHT))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(enhanced_image)\n",
        "            else:\n",
        "                displayJsImage(iteration, f'CodeFormer {iteration}', enhanced_image)\n",
        "            print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "            if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "              UPSCALE_AMOUNT = nearest_value\n",
        "              print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            sr_image = upscale(enhanced_image, UPSCALE_AMOUNT)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage(iteration, f'Real-ESRGAN {iteration}', sr_image)\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            clean_env()\n",
        "    else:\n",
        "        if not INSTALL_CODEFORMER:\n",
        "            print(\"CodeFormer is not installed! Please check INSTALL_CODEFORMER and run the environment setup again.\")\n",
        "        if not INSTALL_ESRGAN:\n",
        "            print(\"Real-ESRGAN is not installed! Please check INSTALL_ESRGAN and run the environment setup again.\")\n",
        "      \n",
        "# End Diffuse Function\n",
        "\n",
        "# Setup Prompts\n",
        "if PROMPT.lower() in [None, '', 'none'] and PROMPT_FILE in [None, '', 'none']:\n",
        "    raise Exception(\"PROMPT and PROMPT_FILE are empty! You need to provide a PROMPT or PROMPT_FILE!\")\n",
        "\n",
        "PROMPTS = []\n",
        "if PROMPT_FILE not in ['','none']:\n",
        "    try:\n",
        "        with open(PROMPT_FILE, \"r\") as f:\n",
        "            PROMPTS = f.read().splitlines()\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "\n",
        "if PROMPT not in ['', 'none']:\n",
        "    PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "#Get corrected sizes\n",
        "WX = (WIDTH//64)*64;\n",
        "HY = (HEIGHT//64)*64;\n",
        "if WX is not WIDTH or HY is not HEIGHT:\n",
        "    #print(f'Changing output size to {WX}x{HY}. Dimensions must by multiples of 64.')\n",
        "    WIDTH = WX\n",
        "    HEIGHT = HY\n",
        "\n",
        "# Setup init_iamge\n",
        "init = None\n",
        "if INIT_IMAGE.lower() not in [None, '', 'none']:\n",
        "    from PIL import ImageOps\n",
        "    init = Image.open(fetch(INIT_IMAGE)).convert(\"RGB\")\n",
        "    init = ImageOps.exif_transpose(init)\n",
        "    init = init.resize((WIDTH,HEIGHT))\n",
        "    original_init = init\n",
        "    init = preprocess(init)\n",
        "\n",
        "\n",
        "# Initiate non-cached pipelines\n",
        "if not CACHE_PIPELINES:\n",
        "    print(\"Setting up diffusion model pipeline...\")\n",
        "    try:\n",
        "        if pipe:\n",
        "            del pipe\n",
        "    except NameError:\n",
        "        pass\n",
        "    if LOW_VRAM_PATCH and init is None:\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "        del pipe.vae.encoder\n",
        "    elif init is not None:\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, cache_dir=model_cache, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    else:\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, use_auth_token=True).to(\"cuda\")\n",
        "\n",
        "with precision_scope(\"cuda\"):\n",
        "\n",
        "    for pi in PROMPTS:\n",
        "\n",
        "        # Define Run Prompt\n",
        "        if NEW_NSP_ON_ITERATION is not True:\n",
        "            PROMPT = nsp_parse(pi)\n",
        "            epoch_time = int(time.time())\n",
        "            if SAVE_PROMPT_DETAILS:\n",
        "                with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "\n",
        "        for iteration in range(NUM_ITERS):\n",
        "\n",
        "            # Define Iteration Prompt\n",
        "            if NEW_NSP_ON_ITERATION:\n",
        "                PROMPT = nsp_parse(pi)\n",
        "                epoch_time = int(time.time())\n",
        "                if SAVE_PROMPT_DETAILS:\n",
        "                    with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                            file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "\n",
        "            try:\n",
        "\n",
        "                diffuse_run()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if 'out of memory' in str(e):\n",
        "                    print(f\"\\u001b[31m\\u001b[1m\\u001b[4mCRITICAL ERROR\\u001b[0m: {gpu_name} ran out of memory! Flushing memory to save session...\")\n",
        "                    pass\n",
        "                else:\n",
        "                    raise e\n",
        "            except KeyboardInterrupt as e:\n",
        "                print('\\nSeed used for this exited run:', SEED)\n",
        "                raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "            finally:\n",
        "                print(\"Cleaning up...\\n\")\n",
        "                clean_env()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Stability.AI Easy Diffusion CA_WIP",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}